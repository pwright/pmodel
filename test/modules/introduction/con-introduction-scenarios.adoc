[id="introduction-scenarios"]
= How to use {product}
:imagesdir: ../../_images

[role="_abstract"]
Use {product} to create, manage and check the status of Kafka instances, topics, and consumer groups.
You can perform these activities from the console, the `rhoas` CLI, or REST API.

Create a Kafka instance and a service account that provides the details for connecting to that Kafka instance.
With the Kafka instance running, create and manage topics.
Use the service account connection details to handle user requests from Kafka client applications to produce or consume messages from those topics.

== Create Kafka instances

When you first use the {product} service, you’ll begin by creating a Kafka instance.
After the instance is created, you're ready to create topics.

To send or consume messages to topics, you can look up the bootstrap server
and generate a service account that can be used by an application to connect to the Kafka instance.

== Create service accounts

A service account is like a user account, but for client applications.
You create a service account to allow client applications to connect to your Kafka instance.
When you first create a service account, the credentials required for connection — a client ID and client secret — are generated for the account.

You can reset the credentials or delete the service account at any time.
However, you can only access the client ID and secret when you first create a service account.
Make sure to record these details during the creation process for later use when connecting producer or consumer applications to the Kafka instance.

== Create and configure topics

When you create a topic, you set core configuration, such as:

* The number of topic partitions
* Size-based and time-based message retention policies

You can also define configuration for:

* Message size and compression type
* Log indexing, and cleanup and flushing of old data

From the web console, you can view all the topics created for a Kafka instance. You can then select and delete any topics.

NOTE: For topic replication, partition leader elections can be clean or unclean.
{product} allows only clean leader election, which means that out-of-sync replicas cannot become leaders.
If no in-sync replicas are available, Kafka waits until the original leader is back online before messages are picked up again.

== Set up client access to Kafka instances

You can set up clients and utilities to produce and consume messages from your Kafka instances.
The client must be configured with the connection details required to make a secure connection.

{product} provides both SASL/OAUTHBEARER and SASL/PLAIN for client authentication.

Both mechanisms allow clients to establish authenticated sessions with the Kafka instance. SASL/OAUTHBEARER authentication is the recommended authentication method.

SASL/OAUTHBEARER authentication uses token-based credentials exchange, which means the client never shares its credentials with the Kafka instance.
Support for SASL/OAUTHBEARER authentication is built in to many standard Kafka clients.

If a client doesn’t support SASL/OAUTHBEARER authentication, you can use SASL/PLAIN authentication.
For SASL/PLAIN authentication, the connection details include the client ID and client secret created for the service account and the bootstrap server URL.

== Monitor consumer groups

When you configure an application client to access Kafka, you can assign a group ID to associate the consumer with a consumer group.
All consumers with the same group ID belong to the consumer group with that ID.

From the console, you can view all the consumer groups that have access to a particular Kafka instance.
For each consumer group, you can see the number of active members and its state.
The state might indicate, for example, that the consumer group is stable or empty.

You also get details on the number of partitions with lag.
_Consumer lag_ for a given consumer group indicates the delay between the last message added in a partition and the message currently being picked up by that consumer.
The lag reflects the position of the consumer offset in relation to the end of the partition log.
This difference is sometimes referred to as the delta between the producer offset and consumer offset, which are the read and write positions in the Kafka broker topic partitions.
And it’s a particularly important metric.
For applications that rely on the processing of (near) real-time data, it's critical to monitor consumer lag to check that it doesn't become too big.
The greater the lag becomes, the further the process moves from the real-time processing objective. Lag is often reduced by adding new consumers to a group.

.Consumer lag between the producer and consumer offset
image::introduction/160_OpenShift_Streams_Apache_Kafka_0421_lag.svg[Image of consumer lag shown from partition offset positions]

If you select a specific consumer group in the console, you can view details of the consumers receiving messages from each partition in the topic.
You can also see further consumer status information, such as the current offset and the log end offset, which is the offset of the last message written to the log.

== Bind {osd-name-short}-based applications to the service

{osd-name} is an enterprise Kubernetes platform managed and supported by {org-name}. {osd-name-short} removes the operational complexity of running and maintaining OpenShift on a cloud provider.

If you're running an {osd-name-short}-based client application, you can use the Red Hat Service Binding Operator to bind the application from a given namespace to the {product} service.

Use the `rhoas` CLI `rhoas cluster connect` command to:

* Create a service account and mount it as a secret into your cluster
* Create a Kafka `Request` object to create a `ServiceBinding` object using the Service Binding operator


[role="_additional-resources"]
.Additional resources
* link:{service-url}[cloud.redhat.com^]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a[Getting Started with {product-long}^]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f520e427-cad2-40ce-823d-96234ccbc047[Getting started with the `rhoas` CLI^]
* link:https://api.openshift.com/?urls.primaryName=managed-services-api%20service[API documentation^]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/c0ab8d79-8b74-4876-955d-6d5b6912a966[Using Kafka bin scripts with {product-long}^]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/04827d87-ed92-4ffd-a126-11fa13348eba[Using Quarkus applications with Kafka instances in {product-long}^]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/ee92cfdb-9587-42f8-80d5-54169e0e3c07[Using Kafkacat with Kafka instances in {product-long}^]
* link:{osd-docs}[{osd-name}^]
